{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3597a4-8ec5-4d35-879e-8c8c5721ef90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: #0A6EBD; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 20px; font-size: 40px; font-weight: bold; border-radius: 0 0 0 0; box-shadow: 0px 6px 8px rgba(0, 0, 0, 0.2);\">\n",
    "    FIT-HCMUS, VNU-HCM \n",
    "    <br>\n",
    "    Introduction To Data Science \n",
    "    <br>\n",
    "    Final project ðŸ“Œ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480d656-a992-4d9d-82f0-a1c3b1d53ae7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: #5A96E3; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 20px; font-size: 40px; font-weight: bold; border-radius: 0 0 0 0; box-shadow: 0px 6px 8px rgba(0, 0, 0, 0.2);\">\n",
    "  Stage 01 - Data collecting ðŸ“Œ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da36331-6ea0-4891-8ef5-e079e7eb2c99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data source\n",
    "\n",
    "Data sources we collected for this project include:\n",
    "- ... **TODO** list out some links of source and give bried description description (include where, why and what is that data; and what kind of crawling we use - html parse or api,..etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211782b7-5272-4088-b3f2-3fc3a0caa910",
   "metadata": {},
   "source": [
    "***MAYBE***\n",
    "\n",
    "explain something about data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14da7e9-e364-427e-9cc5-c01006cd4e09",
   "metadata": {},
   "source": [
    "## 2. Set up enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6b5c43-c8d2-445d-a3f9-638326a3f66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2e55a-2226-4ea1-9e6a-b45319d374a8",
   "metadata": {},
   "source": [
    "## 3. Crawl data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBREF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list of club in the league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_club_list_fbref(url,league_path):\n",
    "    url_league = url+league_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    res = requests.get(url_league,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    tables = soup.find_all(\"table\",{\"id\":\"results2023-202491_overall\"})\n",
    "    club_list = []\n",
    "    for t in tables:\n",
    "        rows = t.select(\"tbody tr:not(.thead)\")\n",
    "        for row in rows :\n",
    "            line = row.find(\"td\",{\"data-stat\":\"team\"})\n",
    "            for a in line.find_all('a', href=True):\n",
    "                club_list.append(a['href'])\n",
    "    time.sleep(3)\n",
    "    return club_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate list of players for each team in the league"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_player_list_fbref(url,club_path):\n",
    "    url_club = url+club_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    res = requests.get(url_club,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    tables = soup.find_all(\"table\",{\"id\":\"stats_standard_9\"})\n",
    "    player_list = []\n",
    "    for t in tables:\n",
    "        rows = t.select(\"tbody tr:not(.thead)\")\n",
    "        for row in rows :\n",
    "            line = row.find(\"th\",{\"data-stat\":\"player\"})\n",
    "            for a in line.find_all('a', href=True):\n",
    "                player_list.append(a['href'])\n",
    "    time.sleep(3)\n",
    "    return player_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawl data for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_player_fbref(url,player_path):\n",
    "    url_player = url+player_path\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "    res = requests.get(url_player,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    tables = soup.find_all(\"table\",{\"id\":\"stats_standard_dom_lg\"})\n",
    "    if len(tables) == 0 : \n",
    "        return None\n",
    "    table =tables[0]\n",
    "    header = ['Name'] + [element.getText() for element in table.find(\"thead\").findAll(\"tr\")[1].findAll(\"th\")]\n",
    "    rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "    if len(rows) <= 10:\n",
    "        selected_rows = rows\n",
    "    else:\n",
    "        selected_rows = rows[-10:]\n",
    "    data = []\n",
    "    for row in selected_rows:\n",
    "        playerName = [soup.find(\"h1\").find(\"span\").get_text()]\n",
    "        season_id = [row.find('th').getText()]\n",
    "        line =[data.getText() for data in row.findAll(\"td\") ]\n",
    "        line = playerName+season_id  + line\n",
    "        data.append(line)\n",
    "    df = pd.DataFrame(data=data,columns=header)\n",
    "    time.sleep(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of scraping data from FBREF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b9e3f51-67e2-4254-bedd-85eef66d575a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crawl_fbref(url,league_path):\n",
    "    club_list = generate_club_list_fbref(url,league_path)\n",
    "    player_list =[]\n",
    "    for club in club_list:\n",
    "        club_player_list = generate_player_list_fbref(url,club)\n",
    "        player_list = player_list+club_player_list\n",
    "    main_df = crawl_player_fbref(url,player_list[0])\n",
    "    for player in player_list[1:]:\n",
    "        sub_df = crawl_player_fbref(url,player)\n",
    "        if sub_df is None : continue\n",
    "        try:\n",
    "            main_df = pd.concat([main_df, sub_df], ignore_index=True, axis=0)   \n",
    "        except pd.errors.InvalidIndexError:\n",
    "            print(\"Skipping player because player doesn't have data in first team\")\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n"
     ]
    }
   ],
   "source": [
    "url = \"https://fbref.com\"\n",
    "league_path = \"/en/comps/9/Premier-League-Stats\"\n",
    "df_fbref = crawl_fbref(url,league_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfermarkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_season(season):\n",
    "    if int(season[:2]) <13 :\n",
    "        return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_club_list_trans(url,league_path):\n",
    "    url_league = url+league_path\n",
    "    # headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    headers = {'User-agent': 'Super Bot Power Level Over 9000'}\n",
    "    res = requests.get(url_league,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    tables = soup.find_all(\"table\",{\"class\":\"items\"})\n",
    "\n",
    "    club_list = []\n",
    "    table = tables[0]\n",
    "    rows = table.select(\"tbody tr:not(.thead)\")\n",
    "    for row in rows :\n",
    "        line = row.find(\"td\",{\"class\":\"hauptlink no-border-links\"})\n",
    "        for a in line.find_all('a', href=True):\n",
    "            if a['href'] != '#':\n",
    "                club_list.append(a['href'])\n",
    "    time.sleep(3)\n",
    "    return club_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_player_list_trans(url,club_path):\n",
    "    url_club = url+club_path\n",
    "    # headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    headers = {'User-agent': 'Super Bot Power Level Over 9000'}\n",
    "    res = requests.get(url_club,headers=headers)\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    tables = soup.find_all(\"table\",{\"class\":\"items\"})\n",
    "    table =tables[0]\n",
    "    player_list = []\n",
    "    rows = table.select(\"tbody tr\")\n",
    "    for row in rows:\n",
    "        line = row.find(\"td\",{\"class\":\"hauptlink\"})\n",
    "        if line is not None : \n",
    "            a_tag = line.find('a', href=True)\n",
    "            if a_tag and a_tag['href'] not in player_list:\n",
    "                player_list.append(a_tag['href'])\n",
    "    time.sleep(3)\n",
    "    return player_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_injury_data_player_page(url,player_path):\n",
    "    url_player = url+player_path\n",
    "    # headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    headers = {'User-agent': 'Super Bot Power Level Over 9000'}\n",
    "    res = requests.get(url_player,headers=headers)\n",
    "    soup=BeautifulSoup(res.text, \"html.parser\")\n",
    "    name_tag = soup.find(\"div\",{\"class\":\"data-header__headline-container\"})\n",
    "    name = [name_tag.find(\"strong\").getText()]\n",
    "    tables = soup.find_all(\"table\",{\"class\":\"items\"})\n",
    "    if (len(tables) == 0) : \n",
    "        print(\"There is no injury data for this player \")\n",
    "        return None\n",
    "    table = tables[0]\n",
    "\n",
    "    header = ['Name'] + [element.getText() for element in table.find(\"thead\").findAll(\"tr\")[0].findAll(\"th\")]\n",
    "    full_data =[]\n",
    "    rows = table.select(\"tbody tr\")\n",
    "    for row in rows:\n",
    "        line =[data.getText() for data in row.findAll(\"td\") ]\n",
    "        if not check_season(line[0]) : continue \n",
    "        line = name+ line\n",
    "        full_data.append(line)\n",
    "    df = pd.DataFrame(data=full_data,columns=header)\n",
    "    time.sleep(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_injury_data_player(url,player_path):\n",
    "    player_path = player_path.replace(\"profil\", \"verletzungen\")\n",
    "    url_player = url+player_path\n",
    "    # headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    headers = {'User-agent': 'Super Bot Power Level Over 9000'}\n",
    "    res = requests.get(url_player,headers=headers)\n",
    "    soup=BeautifulSoup(res.text, \"html.parser\")\n",
    "    page = soup.find(\"div\",{\"class\":\"pager\"})\n",
    "    if page is None : \n",
    "        return crawl_injury_data_player_page(url,player_path)\n",
    "    page_link = page.find_all(\"li\",{\"class\":\"tm-pagination__list-item\"})\n",
    "    links =[]\n",
    "    for link in page_link :\n",
    "        line = link.find('a', href=True)\n",
    "        if line['href'] is None or line['href'] in links:\n",
    "            continue\n",
    "        links.append(line['href'])    \n",
    "    main_df = crawl_injury_data_player_page(url,links[0])\n",
    "    for link in links[1:]:\n",
    "        sub_df = crawl_injury_data_player_page(url,link)\n",
    "        if sub_df is None : \n",
    "            continue\n",
    "        main_df= pd.concat([main_df,sub_df],ignore_index=True,axis = 0)\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_transfermarkt(url,league_path):\n",
    "    club_list = generate_club_list_trans(url,league_path)\n",
    "    player_list =[]\n",
    "    for club in club_list:\n",
    "        print(club)\n",
    "        club_player_list = generate_player_list_trans(url,club)\n",
    "        player_list = player_list+club_player_list\n",
    "    main_df = crawl_injury_data_player(url,player_list[0])\n",
    "    for player in player_list[1:]:\n",
    "        print(player)\n",
    "        sub_df = crawl_injury_data_player(url,player)\n",
    "        if sub_df is None : continue\n",
    "        main_df = pd.concat([main_df, sub_df], ignore_index=True, axis=0)   \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_trans= \"https://www.transfermarkt.com\"\n",
    "league_path_trans = \"/premier-league/startseite/wettbewerb/GB1\"\n",
    "df_trans = crawl_transfermarkt(url_trans,league_path_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c48b13-a36d-4891-ab87-918df8f0241a",
   "metadata": {},
   "source": [
    "## 4. Clean and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ddb3330-0d08-44c8-9ee3-981542679e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TODO\n",
    "df_fbref.to_csv('../../data/raw_data_fbref.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
