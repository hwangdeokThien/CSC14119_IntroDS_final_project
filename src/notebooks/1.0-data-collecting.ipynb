{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee3597a4-8ec5-4d35-879e-8c8c5721ef90",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: #0A6EBD; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 20px; font-size: 40px; font-weight: bold; border-radius: 0 0 0 0; box-shadow: 0px 6px 8px rgba(0, 0, 0, 0.2);\">\n",
    "    FIT-HCMUS, VNU-HCM \n",
    "    <br>\n",
    "    Introduction To Data Science \n",
    "    <br>\n",
    "    Final project ðŸ“Œ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1480d656-a992-4d9d-82f0-a1c3b1d53ae7",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; background-color: #5A96E3; font-family: 'Trebuchet MS', Arial, sans-serif; color: white; padding: 20px; font-size: 40px; font-weight: bold; border-radius: 0 0 0 0; box-shadow: 0px 6px 8px rgba(0, 0, 0, 0.2);\">\n",
    "  Stage 01 - Data collecting ðŸ“Œ\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da36331-6ea0-4891-8ef5-e079e7eb2c99",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Data source\n",
    "\n",
    "Data sources we collected for this project include:\n",
    "- [FBref](https://fbref.com/en/): This is the website which contains football data through each season. The reason why we choose this website is that this website has many kinds of data from general data to detailed data , from each team to each player. The data we choose to scrape in this website is the standard stats in the latest 10 seasons for each player in The Premier League 2023/2024 because we think the kind of stat can provide enough information for analysing and predicting in our project. We scrape this website by sending requests to the server of website and parsing HTML text \n",
    "- [Transfermarkt](https://www.transfermarkt.com/) : This is also the website which contains football data through each season. The reason why we choose this website is that this website has data about player's injury which is a necessary part in our projects. The data we choose to scrape in this website is the injury data in the latest 10 seasons for each player in The Premier League 2023/2024. We scrape this website by sending requests to the server of website and parsing HTML text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14da7e9-e364-427e-9cc5-c01006cd4e09",
   "metadata": {},
   "source": [
    "## 2. Set up enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6b5c43-c8d2-445d-a3f9-638326a3f66c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2e55a-2226-4ea1-9e6a-b45319d374a8",
   "metadata": {},
   "source": [
    "## 3. Crawl data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62b9aa3",
   "metadata": {},
   "source": [
    "### FBREF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc6e77c",
   "metadata": {},
   "source": [
    "Generate list of club in the league in Fbref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978894a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_club_list_fbref(url,league_path):\n",
    "    url_league = url+league_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_league,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    #Find the table which contains general information about clubs in the league \n",
    "    tables = soup.find_all(\"table\",{\"id\":\"results2023-202491_overall\"})\n",
    "    club_list = []\n",
    "    #Iterate through each row to get the club's link in the website\n",
    "    for t in tables:\n",
    "        rows = t.select(\"tbody tr:not(.thead)\")\n",
    "        for row in rows :\n",
    "            line = row.find(\"td\",{\"data-stat\":\"team\"})\n",
    "            for a in line.find_all('a', href=True):\n",
    "                club_list.append(a['href'])\n",
    "    time.sleep(3)\n",
    "    return club_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566aca40",
   "metadata": {},
   "source": [
    "Generate list of players for each team in the league in Fbref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "412e28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_player_list_fbref(url,club_path):\n",
    "    url_club = url+club_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_club,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    #Find the table which contains general information about players in the club\n",
    "    tables = soup.find_all(\"table\",{\"id\":\"stats_standard_9\"})\n",
    "    player_list = []\n",
    "    #Iterate through each row to get the player's link in the website\n",
    "    for t in tables:\n",
    "        rows = t.select(\"tbody tr:not(.thead)\")\n",
    "        for row in rows :\n",
    "            line = row.find(\"th\",{\"data-stat\":\"player\"})\n",
    "            for a in line.find_all('a', href=True):\n",
    "                player_list.append(a['href'])\n",
    "    time.sleep(3)\n",
    "    return player_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1fd6b",
   "metadata": {},
   "source": [
    "Crawl data for each player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb3f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_player_fbref(url,player_path):\n",
    "    url_player = url+player_path\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_player,headers=headers)\n",
    "    comm = re.compile(\"<!--|-->\")\n",
    "    soup = BeautifulSoup(comm.sub(\"\",res.text),'html.parser')\n",
    "    #Find the table which contains standard stats about players in their career\n",
    "    tables = soup.find_all(\"table\",{\"id\":\"stats_standard_dom_lg\"})\n",
    "    if len(tables) == 0 : \n",
    "        return None\n",
    "    table =tables[0]\n",
    "    #Get the name of columns in DataFrame\n",
    "    header = ['Name'] + [element.getText() for element in table.find(\"thead\").findAll(\"tr\")[1].findAll(\"th\")]\n",
    "    rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "    #Get the latest 10 seasons in their career \n",
    "    if len(rows) <= 10:\n",
    "        selected_rows = rows\n",
    "    else:\n",
    "        selected_rows = rows[-10:]\n",
    "    data = []\n",
    "    #Iterate each row to get data in DataFrame\n",
    "    for row in selected_rows:\n",
    "        playerName = [soup.find(\"h1\").find(\"span\").get_text()]\n",
    "        season_id = [row.find('th').getText()]\n",
    "        line =[data.getText() for data in row.findAll(\"td\") ]\n",
    "        line = playerName+season_id  + line\n",
    "        data.append(line)\n",
    "    df = pd.DataFrame(data=data,columns=header)\n",
    "    time.sleep(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b27668",
   "metadata": {},
   "source": [
    "The process of scraping data from Fbref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9e3f51-67e2-4254-bedd-85eef66d575a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def crawl_fbref(url,league_path):\n",
    "    club_list = generate_club_list_fbref(url,league_path)\n",
    "    player_list =[]\n",
    "    for club in club_list:\n",
    "        club_player_list = generate_player_list_fbref(url,club)\n",
    "        player_list = player_list+club_player_list\n",
    "    main_df = crawl_player_fbref(url,player_list[0])\n",
    "    for player in player_list[1:]:\n",
    "        sub_df = crawl_player_fbref(url,player)\n",
    "        if sub_df is None : continue\n",
    "        try:\n",
    "            main_df = pd.concat([main_df, sub_df], ignore_index=True, axis=0)   \n",
    "        except pd.errors.InvalidIndexError:\n",
    "            print(\"Skipping player because player doesn't have data in first team\")\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b8b4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n",
      "Skipping player because player doesn't have data in first team\n"
     ]
    }
   ],
   "source": [
    "url = \"https://fbref.com\"\n",
    "league_path = \"/en/comps/9/Premier-League-Stats\"\n",
    "df_fbref = crawl_fbref(url,league_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3270fa",
   "metadata": {},
   "source": [
    "### Transfermarkt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab07611",
   "metadata": {},
   "source": [
    "Check the season in the range 10 years (The season now : 23/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c2fbaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_season(season):\n",
    "    if int(season[:2]) <13 :\n",
    "        return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c29eccf",
   "metadata": {},
   "source": [
    "Generate list of clubs in Transfermarkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c8cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_club_list_trans(url,league_path):\n",
    "    url_league = url+league_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_league,headers=headers)\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    #Find the table which contains general information about clubs in the league\n",
    "    tables = soup.find_all(\"table\",{\"class\":\"items\"})\n",
    "\n",
    "    club_list = []\n",
    "    table = tables[0]\n",
    "    rows = table.select(\"tbody tr:not(.thead)\")\n",
    "    #Iterate through each row to get the club's link in the website\n",
    "    for row in rows :\n",
    "        line = row.find(\"td\",{\"class\":\"hauptlink no-border-links\"})\n",
    "        for a in line.find_all('a', href=True):\n",
    "            if a['href'] != '#':\n",
    "                club_list.append(a['href'])\n",
    "    time.sleep(3)\n",
    "    return club_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca8892",
   "metadata": {},
   "source": [
    "Generate list of players for each clubs in Transfermarkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f57462",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_player_list_trans(url,club_path):\n",
    "    url_club = url+club_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_club,headers=headers)\n",
    "    soup = BeautifulSoup(res.text,'html.parser')\n",
    "    #Find the table which contains general information about players in the club\n",
    "    tables = soup.find_all(\"table\",{\"class\":\"items\"})\n",
    "    table =tables[0]\n",
    "    player_list = []\n",
    "    rows = table.select(\"tbody tr\")\n",
    "    #Iterate through each row to get the player's link in the website\n",
    "    for row in rows:\n",
    "        line = row.find(\"td\",{\"class\":\"hauptlink\"})\n",
    "        if line is not None : \n",
    "            a_tag = line.find('a', href=True)\n",
    "            if a_tag and a_tag['href'] not in player_list:\n",
    "                player_list.append(a_tag['href'])\n",
    "    time.sleep(3)\n",
    "    return player_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20992b9",
   "metadata": {},
   "source": [
    "Scrape injury data of each player for 1 page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5847f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_injury_data_player_page(url,player_path):\n",
    "    url_player = url+player_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_player,headers=headers)\n",
    "    soup=BeautifulSoup(res.text, \"html.parser\")\n",
    "    name_tag = soup.find(\"div\",{\"class\":\"data-header__headline-container\"})\n",
    "    #Find the name of the player\n",
    "    name = name_tag.find(\"h1\",{\"class\":\"data-header__headline-wrapper\"}).getText()\n",
    "    name = re.sub(r\"[^\\w\\s]\", \"\", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    name = re.sub(r\"\\d+\", \"\", name)\n",
    "    name = [name.strip()]\n",
    "    #Find the table which contains injury data \n",
    "    tables = soup.find_all(\"table\",{\"class\":\"items\"})\n",
    "    if (len(tables) == 0) : \n",
    "        print(\"There is no injury data for this player \")\n",
    "        return None\n",
    "    table = tables[0]\n",
    "    #Get the name of columns in DataFrame\n",
    "    header = ['Name'] + [element.getText() for element in table.find(\"thead\").findAll(\"tr\")[0].findAll(\"th\")]\n",
    "    full_data =[]\n",
    "    rows = table.select(\"tbody tr\")\n",
    "    #Iterate each row to get data in DataFrame\n",
    "    for row in rows:\n",
    "        line =[data.getText() for data in row.findAll(\"td\") ]\n",
    "        if not check_season(line[0]) : continue \n",
    "        line = name+ line\n",
    "        full_data.append(line)\n",
    "    df = pd.DataFrame(data=full_data,columns=header)\n",
    "    time.sleep(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2bf5d1",
   "metadata": {},
   "source": [
    "Scrape injury data for each player in all pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9955fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_injury_data_player(url,player_path):\n",
    "    #Change page from Profile to Injury History  \n",
    "    player_path = player_path.replace(\"profil\", \"verletzungen\")\n",
    "    url_player = url+player_path\n",
    "    headers = {'User-agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.186 Safari/537.36'}\n",
    "    #Send the request to the website\n",
    "    res = requests.get(url_player,headers=headers)\n",
    "    soup=BeautifulSoup(res.text, \"html.parser\")\n",
    "    #Find the number of pages in the table\n",
    "    page = soup.find(\"div\",{\"class\":\"pager\"})\n",
    "    #Scrape injury data for all pages\n",
    "    if page is None : \n",
    "        return crawl_injury_data_player_page(url,player_path)\n",
    "    page_link = page.find_all(\"li\",{\"class\":\"tm-pagination__list-item\"})\n",
    "    links =[]\n",
    "    for link in page_link :\n",
    "        line = link.find('a', href=True)\n",
    "        if line['href'] is None or line['href'] in links:\n",
    "            continue\n",
    "        links.append(line['href'])    \n",
    "    main_df = crawl_injury_data_player_page(url,links[0])\n",
    "    for link in links[1:]:\n",
    "        sub_df = crawl_injury_data_player_page(url,link)\n",
    "        if sub_df is None : \n",
    "            continue\n",
    "        main_df= pd.concat([main_df,sub_df],ignore_index=True,axis = 0)\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956a6b0",
   "metadata": {},
   "source": [
    "The process of scraping injury data from Transfermarkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c83407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_transfermarkt(url,league_path):\n",
    "    club_list = generate_club_list_trans(url,league_path)\n",
    "    player_list =[]\n",
    "    for club in club_list:\n",
    "        club_player_list = generate_player_list_trans(url,club)\n",
    "        player_list = player_list+club_player_list\n",
    "    main_df = crawl_injury_data_player(url,player_list[0])\n",
    "    for player in player_list[1:]:\n",
    "        sub_df = crawl_injury_data_player(url,player)\n",
    "        if sub_df is None : continue\n",
    "        main_df = pd.concat([main_df, sub_df], ignore_index=True, axis=0)   \n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6c35192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n",
      "There is no injury data for this player \n"
     ]
    }
   ],
   "source": [
    "url_trans= \"https://www.transfermarkt.com\"\n",
    "league_path_trans = \"/premier-league/startseite/wettbewerb/GB1\"\n",
    "df_trans = crawl_transfermarkt(url_trans,league_path_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c48b13-a36d-4891-ab87-918df8f0241a",
   "metadata": {},
   "source": [
    "## 4. Clean and save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ddb3330-0d08-44c8-9ee3-981542679e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Save the fbref data in csv\n",
    "df_fbref.to_csv('../../data/raw/raw_data_fbref.csv',index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58d67c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the transfermrkt data in csv\n",
    "df_trans.to_csv('../../data/raw/raw_data_trans.csv',index=False,encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
